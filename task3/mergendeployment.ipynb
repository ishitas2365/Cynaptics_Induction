{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10468802,"sourceType":"datasetVersion","datasetId":6481911},{"sourceId":120005,"sourceType":"modelInstanceVersion","modelInstanceId":100936,"modelId":121027}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U accelerate\n%pip install -U peft\n%pip install -U trl\n%pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:46:14.841729Z","iopub.execute_input":"2025-01-14T15:46:14.842037Z","iopub.status.idle":"2025-01-14T15:46:49.292940Z","shell.execute_reply.started":"2025-01-14T15:46:14.842013Z","shell.execute_reply":"2025-01-14T15:46:49.291845Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"ft_hfhub\")\nlogin(token = hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:46:49.294562Z","iopub.execute_input":"2025-01-14T15:46:49.294922Z","iopub.status.idle":"2025-01-14T15:46:50.084562Z","shell.execute_reply.started":"2025-01-14T15:46:49.294887Z","shell.execute_reply":"2025-01-14T15:46:50.083929Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\n\n# List contents of the dataset directory\ndataset_dir = \"/kaggle/input/checkpoint-2700-llama3-2-3b-it\"\nprint(\"Contents of the dataset directory:\")\nprint(os.listdir(dataset_dir))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:46:50.086311Z","iopub.execute_input":"2025-01-14T15:46:50.086602Z","iopub.status.idle":"2025-01-14T15:46:50.099366Z","shell.execute_reply.started":"2025-01-14T15:46:50.086579Z","shell.execute_reply":"2025-01-14T15:46:50.098509Z"}},"outputs":[{"name":"stdout","text":"Contents of the dataset directory:\n['adapter_model.safetensors', 'trainer_state.json', 'training_args.bin', 'adapter_config.json', 'README.md', 'tokenizer.json', 'tokenizer_config.json', 'scheduler.pt', 'special_tokens_map.json', 'optimizer.pt', 'rng_state.pth']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import PeftModel\nimport torch\n\n# Quantization configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,  # Use 4-bit quantization\n    bnb_4bit_quant_type=\"nf4\",\n)\n\n# Load quantized base model\nbase_model_path = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"  # Adjust to your base model path\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_path,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:48:44.349458Z","iopub.execute_input":"2025-01-14T15:48:44.349936Z","iopub.status.idle":"2025-01-14T15:49:15.414401Z","shell.execute_reply.started":"2025-01-14T15:48:44.349898Z","shell.execute_reply":"2025-01-14T15:49:15.413744Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5d83b3b024d4a81a4b157182b74d3d3"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Resize token embeddings for mismatch\nbase_model.resize_token_embeddings(128258)  # Adjust size to match checkpoint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:49:31.977076Z","iopub.execute_input":"2025-01-14T15:49:31.977385Z","iopub.status.idle":"2025-01-14T15:49:38.226992Z","shell.execute_reply.started":"2025-01-14T15:49:31.977359Z","shell.execute_reply":"2025-01-14T15:49:38.226200Z"}},"outputs":[{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Embedding(128258, 3072)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)\ntokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:49:38.228111Z","iopub.execute_input":"2025-01-14T15:49:38.228449Z","iopub.status.idle":"2025-01-14T15:49:38.741665Z","shell.execute_reply.started":"2025-01-14T15:49:38.228413Z","shell.execute_reply":"2025-01-14T15:49:38.740754Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from peft import PeftConfig\n\n# Path to the LoRA adapter\nadapter_path = \"/kaggle/input/checkpoint-2700-llama3-2-3b-it\"\nadapter_config = PeftConfig.from_pretrained(adapter_path)\n# Load the LoRA adapter\nmodel = PeftModel.from_pretrained(base_model, adapter_path)\n\n# Set the model to evaluation mode\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:49:38.742995Z","iopub.execute_input":"2025-01-14T15:49:38.743297Z","iopub.status.idle":"2025-01-14T15:50:01.744866Z","shell.execute_reply.started":"2025-01-14T15:49:38.743260Z","shell.execute_reply":"2025-01-14T15:50:01.744037Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(128258, 3072)\n        (layers): ModuleList(\n          (0-27): 28 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=3072, out_features=128258, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Merge the adapter weights into the base model\nmodel = model.merge_and_unload()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:50:01.746178Z","iopub.execute_input":"2025-01-14T15:50:01.746477Z","iopub.status.idle":"2025-01-14T15:50:15.147309Z","shell.execute_reply.started":"2025-01-14T15:50:01.746455Z","shell.execute_reply":"2025-01-14T15:50:15.146155Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"merged_model_path = \"/kaggle/working/merged_ft_llama_model\"\nmodel.save_pretrained(merged_model_path)\ntokenizer.save_pretrained(merged_model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:50:15.148325Z","iopub.execute_input":"2025-01-14T15:50:15.148662Z","iopub.status.idle":"2025-01-14T15:50:20.140623Z","shell.execute_reply.started":"2025-01-14T15:50:15.148626Z","shell.execute_reply":"2025-01-14T15:50:20.139863Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/merged_ft_llama_model/tokenizer_config.json',\n '/kaggle/working/merged_ft_llama_model/special_tokens_map.json',\n '/kaggle/working/merged_ft_llama_model/tokenizer.json')"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(merged_model_path).to(\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(merged_model_path)\ntokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:50:20.141380Z","iopub.execute_input":"2025-01-14T15:50:20.141627Z","iopub.status.idle":"2025-01-14T15:50:22.388750Z","shell.execute_reply.started":"2025-01-14T15:50:20.141606Z","shell.execute_reply":"2025-01-14T15:50:22.387881Z"}},"outputs":[{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"Persona B's characteristics: My name is David, and I'm a 35-year-old math teacher. \"\n                   \"I like to hike and spend time in nature. I'm married with two kids.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Morning! I think I saw you at the parent meeting, what's your name?\"\n    }\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:50:22.389884Z","iopub.execute_input":"2025-01-14T15:50:22.390197Z","iopub.status.idle":"2025-01-14T15:50:22.394254Z","shell.execute_reply.started":"2025-01-14T15:50:22.390168Z","shell.execute_reply":"2025-01-14T15:50:22.393385Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:50:22.396594Z","iopub.execute_input":"2025-01-14T15:50:22.396843Z","iopub.status.idle":"2025-01-14T15:50:22.416571Z","shell.execute_reply.started":"2025-01-14T15:50:22.396822Z","shell.execute_reply":"2025-01-14T15:50:22.415775Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:50:22.417921Z","iopub.execute_input":"2025-01-14T15:50:22.418231Z","iopub.status.idle":"2025-01-14T15:50:22.435134Z","shell.execute_reply.started":"2025-01-14T15:50:22.418198Z","shell.execute_reply":"2025-01-14T15:50:22.434480Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"with torch.no_grad():\n    outputs = model.generate(\n        input_ids=inputs.input_ids,\n        attention_mask=inputs.attention_mask,\n        max_length=200,\n        num_return_sequences=1,\n        temperature=0.8,  # Adjust the randomness\n        top_p=0.9        # Nucleus sampling\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T16:08:29.454888Z","iopub.execute_input":"2025-01-14T16:08:29.455227Z","iopub.status.idle":"2025-01-14T16:08:31.303438Z","shell.execute_reply.started":"2025-01-14T16:08:29.455198Z","shell.execute_reply":"2025-01-14T16:08:31.302494Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Decode the response from the model\ndecoded_text = tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T16:08:32.086073Z","iopub.execute_input":"2025-01-14T16:08:32.086385Z","iopub.status.idle":"2025-01-14T16:08:32.090496Z","shell.execute_reply.started":"2025-01-14T16:08:32.086362Z","shell.execute_reply":"2025-01-14T16:08:32.089669Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Extract the assistant's reply\nif \"assistant\" in decoded_text:\n    response = decoded_text.split(\"assistant\", 1)[1].strip()\nelse:\n    response = decoded_text.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T16:08:35.234925Z","iopub.execute_input":"2025-01-14T16:08:35.235253Z","iopub.status.idle":"2025-01-14T16:08:35.239091Z","shell.execute_reply.started":"2025-01-14T16:08:35.235226Z","shell.execute_reply":"2025-01-14T16:08:35.238209Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Print the assistant's reply\nprint(\"Assistant's Reply:\", response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T16:08:37.488610Z","iopub.execute_input":"2025-01-14T16:08:37.488951Z","iopub.status.idle":"2025-01-14T16:08:37.493297Z","shell.execute_reply.started":"2025-01-14T16:08:37.488924Z","shell.execute_reply":"2025-01-14T16:08:37.492440Z"}},"outputs":[{"name":"stdout","text":"Assistant's Reply: Good morning! Yeah, I was at the parent meeting. My name's David, nice to meet you. I'm a math teacher here at the school. How about you, do you have kids in the school?\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"from huggingface_hub import HfApi, HfFolder\n\n# Replace with your repo's path on Hugging Face\nrepo_name = \"ishitas2365/llama-3.2-3b-instruct-finetunedToPersona\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:50:24.569991Z","iopub.execute_input":"2025-01-14T15:50:24.570237Z","iopub.status.idle":"2025-01-14T15:50:24.585867Z","shell.execute_reply.started":"2025-01-14T15:50:24.570218Z","shell.execute_reply":"2025-01-14T15:50:24.585059Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Upload the model\napi = HfApi()\napi.upload_folder(\n    folder_path=merged_model_path, \n    repo_id=repo_name, \n    commit_message=\"Initial model upload\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T15:50:24.586601Z","iopub.execute_input":"2025-01-14T15:50:24.586822Z","iopub.status.idle":"2025-01-14T15:51:30.090357Z","shell.execute_reply.started":"2025-01-14T15:50:24.586803Z","shell.execute_reply":"2025-01-14T15:51:30.089598Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"897a6c6ddaa942b2b10f10213d2f7b1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.37G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b43265c012b8421eb3e9416d0669d6b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c6cb31ed38c4e0088fb0ad34ec025c1"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/ishitas2365/llama-3.2-3b-instruct-finetunedToPersona/commit/aef2b84c3e2a287ceeb3d4d86940cb996bac71b9', commit_message='Initial model upload', commit_description='', oid='aef2b84c3e2a287ceeb3d4d86940cb996bac71b9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ishitas2365/llama-3.2-3b-instruct-finetunedToPersona', endpoint='https://huggingface.co', repo_type='model', repo_id='ishitas2365/llama-3.2-3b-instruct-finetunedToPersona'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":21}]}